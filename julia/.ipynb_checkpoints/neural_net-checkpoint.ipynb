{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Knet,Compat,Plots;\n",
    "include(\"../julia/test_train_split.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4587)\n",
      "(1, 4587)\n",
      "(5, 1147)\n",
      "(1, 1147)\n"
     ]
    }
   ],
   "source": [
    "# file, training_proportion, num_input_stops, stops_ahead_to_predict, stops_to_predict\n",
    "#routes = get_routes(\"/Users/Cooper/Desktop/mbta_trajectories_2014_13.json\",0.6,9,0,1,[],false);\n",
    "path = \"/Volumes/Infinity/mbta/h5/2015/\"\n",
    "key = \"json\"\n",
    "files = map(x->string(path,x),filter(x->contains(x,key), readdir(path)))\n",
    "#files = files[2:3]\n",
    "routes = get_routes(files,0.8,5,0,1,[],true);\n",
    "println(size(routes[\"train_input_data\"]))\n",
    "println(size(routes[\"train_output_data\"]))\n",
    "println(size(routes[\"test_input_data\"]))\n",
    "println(size(routes[\"test_output_data\"]))\n",
    "\n",
    "x_train  = routes[\"train_input_data\"];\n",
    "y_train = routes[\"train_output_data\"];\n",
    "x_test = routes[\"test_input_data\"];\n",
    "y_test = routes[\"test_output_data\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp(w,x)\n",
    "    for i=1:2:length(w)\n",
    "        # w is a vector of weight matrices\n",
    "        x = w[i]*x .+ w[i+1]\n",
    "        if i<length(w)-1\n",
    "            # Apply ReLU nonlinearity\n",
    "            x = max.(0,x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "loss(w,x,y)=0.5*(sum((y-mlp(w,x)).^2) / size(x,2))\n",
    "\n",
    "mlplossgradient = grad(loss);\n",
    "\n",
    "function train(w, dtrn; lr=.5, epochs=10, grad=lossgradient)\n",
    "    for epoch=1:epochs\n",
    "        for (x,y) in dtrn\n",
    "            g = grad(w, x, y)\n",
    "            Knet.update!(w,g,repmat([Sgd(lr=lr)],length(w)))\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end\n",
    "\n",
    "function weights(h)\n",
    "    w = Any[]\n",
    "    x = size(x_train)[1]\n",
    "    for y in vcat(h,size(y_train)[1])\n",
    "        push!(w,0.1*randn(y,x))\n",
    "        push!(w, zeros(y, 1))\n",
    "        x = y\n",
    "    end\n",
    "    return w\n",
    "end\n",
    "\n",
    "\n",
    "function mse(w, dtst; pred=mlp)\n",
    "    s = 0\n",
    "    n = 0\n",
    "    yvar = var(y_test)\n",
    "    for (x, ygold) in dtst\n",
    "        ypred = pred(w, x)\n",
    "        s+= (ypred - ygold).^2\n",
    "        n+=1\n",
    "    end\n",
    "    s_tot =yvar*n\n",
    "    return sqrt.(s/n)\n",
    "    #return (sqrt.(s/n),1-(s/s_tot))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \"Train MAE: \", 70.47200931087458, \"Test MAE: \", 61.89572826014345)\n",
      "(1, \"Train MAE: \", 39.1092688410639, \"Test MAE: \", 42.90929102905879)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m_methods\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ANY, ::ANY, ::Int64, ::UInt64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./reflection.jl:502\u001b[22m\u001b[22m",
      " [2] \u001b[1mreturn_type\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::ANY, ::ANY\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./inference.jl:5465\u001b[22m\u001b[22m",
      " [3] \u001b[1mbroadcast_c\u001b[22m\u001b[22m at \u001b[1m./broadcast.jl:310\u001b[22m\u001b[22m [inlined]",
      " [4] \u001b[1mbroadcast\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Int64, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/unfuse.jl:35\u001b[22m\u001b[22m",
      " [5] \u001b[1mbroadcast#max\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Int64, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/unfuse.jl:48\u001b[22m\u001b[22m",
      " [6] \u001b[1m(::AutoGrad.##rfun#7#10{AutoGrad.#broadcast#max})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Int64, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:123\u001b[22m\u001b[22m",
      " [7] \u001b[1mmax\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/unfuse.jl:52\u001b[22m\u001b[22m [inlined]",
      " [8] \u001b[1m(::##440#441)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Broadcasted{AutoGrad.Rec{Array{Float64,2}}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [9] \u001b[1mbroadcast\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::AutoGrad.Rec{Array{Float64,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/unfuse.jl:37\u001b[22m\u001b[22m",
      " [10] \u001b[1mmlp\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Any,1}}, ::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[118]:7\u001b[22m\u001b[22m",
      " [11] \u001b[1mloss\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Any,1}}, ::Array{Float64,1}, ::Float64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[118]:13\u001b[22m\u001b[22m",
      " [12] \u001b[1mforward_pass\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Tuple{Array{Any,1},Array{Float64,1},Float64}, ::Array{Any,1}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:88\u001b[22m\u001b[22m",
      " [13] \u001b[1m(::AutoGrad.##gradfun#1#3{#loss,Int64})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Array{Any,1}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:39\u001b[22m\u001b[22m",
      " [14] \u001b[1m(::AutoGrad.#gradfun#2)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:39\u001b[22m\u001b[22m",
      " [15] \u001b[1m#train#444\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Float64, ::Int64, ::Function, ::Function, ::Array{Any,1}, ::Base.Iterators.Zip2{Array{Array{Float64,1},1},Array{Float64,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[118]:20\u001b[22m\u001b[22m",
      " [16] \u001b[1m(::#kw##train)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::#train, ::Array{Any,1}, ::Base.Iterators.Zip2{Array{Array{Float64,1},1},Array{Float64,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [17] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[119]:16\u001b[22m\u001b[22m [inlined]",
      " [18] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [19] \u001b[1mmain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[119]:15\u001b[22m\u001b[22m",
      " [20] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "function main()\n",
    "    hidden_layer_sizes = [15,10]\n",
    "    w = weights(hidden_layer_sizes)\n",
    "    dtrn = zip([x_train[:,i] for i in 1:size(x_train,2)],y_train)\n",
    "    dtst = zip([x_test[:,i] for i in 1:size(x_test,2)],y_test)\n",
    "    epochs = 25\n",
    "    learning_rate = 0.0001\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_loss = mse(w,dtrn)\n",
    "    test_loss = mse(w,dtst)\n",
    "    println((0,\"Train MAE: \",train_loss[1],\"Test MAE: \",test_loss[1]))\n",
    "    push!(train_losses,train_loss[1])\n",
    "    push!(test_losses,test_loss[1])\n",
    "    @time for epoch=1:epochs\n",
    "        train(w, dtrn; lr=learning_rate, epochs=1, grad=mlplossgradient)\n",
    "        train_loss = mse(w,dtrn)\n",
    "        test_loss = mse(w,dtst)\n",
    "        println((epoch,\"Train MAE: \",train_loss[1],\"Test MAE: \",test_loss[1]))\n",
    "        push!(train_losses,train_loss[1])\n",
    "        push!(test_losses,test_loss[1])\n",
    "#       gradcheck(loss, w, first(dtrn)...; gcheck=o[:gcheck], verbose=true)\n",
    "    end\n",
    "    return train_losses,test_losses\n",
    "end\n",
    "\n",
    "train_losses,test_losses = main();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(1:length(train_losses),[train_losses,test_losses],xlabel=\"Epoch\",ylabel=\"Mean average error (s)\",title=\"Training and Test loss for MLP\",label=[\"Train loss\" \"Test loss\"],lw=3)\n",
    "png(string(\"mlp_\",Dates.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: @manipulate not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: @manipulate not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "# Plot the trajectory\n",
    "@manipulate for i = 1:10\n",
    "plot(cumsum(x_train[:,i]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wcnn (generic function with 1 method)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cnn(w,x)\n",
    "  x = reshape(x,(length(x),1,1,1))\n",
    "  n = length(w)-4\n",
    "  for i=1:2:n\n",
    "    println(size(w[i]))\n",
    "    println(size(x))\n",
    "    x = pool(max.(0, conv4(w[i],x) .+ w[i+1]))\n",
    "    println(size(x))\n",
    "    println()\n",
    "  end\n",
    "  x = mat(x)\n",
    "  for i=n+1:2:length(w)-2\n",
    "    x = max.(0, w[i] * x .+ w[i+1])\n",
    "  end\n",
    "  return w[end-1] * x .+ w[end]\n",
    "end\n",
    "\n",
    "loss(w,x,y)=0.5*(sum((y-cnn(w,x)).^2) / size(x,2))\n",
    "cnnlossgradient = grad(loss);\n",
    "function wcnn(n)\n",
    "    #size of filter, 1, number input filters, number filters\n",
    "    wcnn=map(Array{Float64}, [ 0.1*randn(5,1,1,20),  zeros(1,1,20,1), \n",
    "                               0.1*randn(5,1,20,50), zeros(1,1,50,1),\n",
    "                               0.1*randn(50,1200),  zeros(50,1),\n",
    "                               0.1*randn(n,50),  zeros(n,1) ]);\n",
    "end\n",
    "#cnn(w,x_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 1, 20)\n",
      "(5, 1, 1, 1)\n",
      "(1, 1, 20, 1)\n",
      "\n",
      "(5, 1, 10, 50)\n",
      "(1, 1, 20, 1)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mDimensionMismatch(\"\")\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mDimensionMismatch(\"\")\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1m#conv4#315\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Int64, ::Int64, ::Int64, ::Int64, ::Int64, ::Array{Any,1}, ::Function, ::Array{Float64,4}, ::Array{Float64,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/Knet/src/conv.jl:420\u001b[22m\u001b[22m",
      " [2] \u001b[1mconv4\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,4}, ::Array{Float64,4}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/Knet/src/conv.jl:416\u001b[22m\u001b[22m",
      " [3] \u001b[1m(::AutoGrad.##rfun#7#10{Knet.#conv4})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::AutoGrad.Rec{Array{Float64,4}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:124\u001b[22m\u001b[22m",
      " [4] \u001b[1m(::Knet.##conv4#197#206{AutoGrad.#rfun#9})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::AutoGrad.Rec{Array{Float64,4}}, ::AutoGrad.Rec{Array{Float64,4}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [5] \u001b[1mconv4\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Float64,4}}, ::AutoGrad.Rec{Array{Float64,4}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [6] \u001b[1mcnn\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Array{Float64,N} where N,1}}, ::Array{Float64,1}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[31]:7\u001b[22m\u001b[22m",
      " [7] \u001b[1mloss\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::AutoGrad.Rec{Array{Array{Float64,N} where N,1}}, ::Array{Float64,1}, ::Float64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[31]:18\u001b[22m\u001b[22m",
      " [8] \u001b[1mforward_pass\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Function, ::Tuple{Array{Array{Float64,N} where N,1},Array{Float64,1},Float64}, ::Array{Any,1}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:88\u001b[22m\u001b[22m",
      " [9] \u001b[1m(::AutoGrad.##gradfun#1#3{#loss,Int64})\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::Array{Array{Float64,N} where N,1}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/Cooper/.julia/v0.6/AutoGrad/src/core.jl:39\u001b[22m\u001b[22m",
      " [10] \u001b[1m#train#55\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Float64, ::Int64, ::AutoGrad.#gradfun#2, ::Function, ::Array{Array{Float64,N} where N,1}, ::Base.Iterators.Zip2{Array{Array{Float64,1},1},Array{Float64,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[11]:20\u001b[22m\u001b[22m",
      " [11] \u001b[1m(::#kw##train)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::#train, ::Array{Array{Float64,N} where N,1}, ::Base.Iterators.Zip2{Array{Array{Float64,1},1},Array{Float64,2}}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [12] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./In[32]:11\u001b[22m\u001b[22m [inlined]",
      " [13] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m./util.jl:237\u001b[22m\u001b[22m [inlined]",
      " [14] \u001b[1mmain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[32]:10\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "function main()\n",
    "    hidden_layer_sizes = [10,10]\n",
    "    w = wcnn(size(y_train)[1])\n",
    "    dtrn = zip([x_train[:,i] for i in 1:size(x_train,2)],y_train)\n",
    "    dtst = zip([x_test[:,i] for i in 1:size(x_test,2)],y_test)\n",
    "    epochs = 50\n",
    "    learning_rate = 0.0001\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    @time for epoch=1:epochs\n",
    "        train(w, dtrn; lr=learning_rate, epochs=1, grad=cnnlossgradient)\n",
    "        train_loss = mse(w,dtrn;pred=cnn)\n",
    "        test_loss = mse(w,dtst;pred=cnn)\n",
    "        println((epoch,\"Train MAE: \",train_loss[1],\"Test MAE: \",test_loss[1]))\n",
    "        push!(train_losses,train_loss[1])\n",
    "        push!(test_losses,test_loss[1])\n",
    "        #gradcheck(loss, w, first(dtrn)...; gcheck=o[:gcheck], verbose=true)\n",
    "    end\n",
    "    return train_losses,test_losses\n",
    "end\n",
    "\n",
    "train_losses,test_losses = main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot(1:length(train_losses),[train_losses,test_losses],title=\"Training and Test loss for CNN\",label=[\"Train loss\" \"Test loss\"],lw=3)\n",
    "png(string(\"cnn_\",Dates.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1407.5949.pdf\n",
    "function lstm(weight,bias,hidden,cell,input)\n",
    "    gates   = hcat(input,hidden) * weight .+ bias\n",
    "    hsize   = size(hidden,2)\n",
    "    forget  = sigm(gates[:,1:hsize])\n",
    "    ingate  = sigm(gates[:,1+hsize:2hsize])\n",
    "    outgate = sigm(gates[:,1+2hsize:3hsize])\n",
    "    change  = tanh(gates[:,1+3hsize:end])\n",
    "    cell    = cell .* forget + ingate .* change\n",
    "    hidden  = outgate .* tanh(cell)\n",
    "    return (hidden,cell)\n",
    "end\n",
    "\n",
    "function initmodel(atype, hidden, vocab, embed)\n",
    "    init(d...)=atype(xavier(d...))\n",
    "    bias(d...)=atype(zeros(d...))\n",
    "    model = Array(Any, 2*length(hidden)+3)\n",
    "    X = embed\n",
    "    for k = 1:length(hidden)\n",
    "        H = hidden[k]\n",
    "        model[2k-1] = init(X+H, 4H)\n",
    "        model[2k] = bias(1, 4H)\n",
    "        model[2k][1:H] = 1 # forget gate bias = 1\n",
    "        X = H\n",
    "    end\n",
    "    model[end-2] = init(vocab,embed)\n",
    "    model[end-1] = init(hidden[end],vocab)\n",
    "    model[end] = bias(1,vocab)\n",
    "    return model\n",
    "end\n",
    "\n",
    "let blank = nothing; global initstate\n",
    "function initstate(model, batch)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    state = Array(Any, 2*nlayers)\n",
    "    for k = 1:nlayers\n",
    "        bias = model[2k]\n",
    "        hidden = div(length(bias),4)\n",
    "        if typeof(blank)!=typeof(bias) || size(blank)!=(batch,hidden)\n",
    "            blank = fill!(similar(bias, batch, hidden),0)\n",
    "        end\n",
    "        state[2k-1] = state[2k] = blank\n",
    "    end\n",
    "    return state\n",
    "end\n",
    "end\n",
    "\n",
    "function rnn(model, state, input; pdrop=0)\n",
    "    nlayers = div(length(model)-3,2)\n",
    "    newstate = similar(state)\n",
    "    for k = 1:nlayers\n",
    "        input = dropout(input, pdrop)\n",
    "        \n",
    "        (newstate[2k-1],newstate[2k]) = lstm(model[2k-1],model[2k],state[2k-1],state[2k],input)\n",
    "        input = newstate[2k-1]\n",
    "    end\n",
    "    return input,newstate\n",
    "end\n",
    "\n",
    "loss_rnn(w,x,y)=0.5*(sum((y-rnn(w,x)).^2) / size(x,2))\n",
    "lossgradient = grad(loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rnn1(w,x,h)\n",
    "    h = tanh.(w[1]*vcat(x,h) .+ w[2])\n",
    "    y = w[3]*h .+ w[4]\n",
    "    return (y,h)\n",
    "end\n",
    "\n",
    "function rnnpredict(w,x)\n",
    "    h = zeros(10)\n",
    "    for t in 1:length(x)\n",
    "        prediction,h = rnn1(w,x[t],h)\n",
    "    end\n",
    "    return prediction\n",
    "end\n",
    "\n",
    "function rnn_total_loss(w, dtst)\n",
    "    s = 0\n",
    "    n = 0\n",
    "    for (x, ygold) in dtst\n",
    "        s+=rnnloss(w,x,ygold)\n",
    "        n+=1\n",
    "    end\n",
    "    return sqrt.(s/n)\n",
    "end\n",
    "    \n",
    "function rnnloss(w,x,y_gold)\n",
    "    state = zeros(10)\n",
    "    prediction = 0\n",
    "    for t in 1:length(x)\n",
    "        prediction,state = rnn1(w,x[t],state)\n",
    "    end\n",
    "    return (prediction - y_gold)[1]^2\n",
    "end\n",
    "\n",
    "function rnnweights(h,s)\n",
    "    w = Any[]\n",
    "    push!(w,0.1*randn(h,1+s))\n",
    "    push!(w,zeros(h))\n",
    "    push!(w,0.1*randn(1,10))\n",
    "    push!(w,zeros(1))\n",
    "    return w\n",
    "end\n",
    "rnnlossgradient = grad(rnnloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, \"Train MAE: \", 31.279724375597183, \"Test MAE: \", 33.71016513594262)\n",
      "(2, \"Train MAE: \", 31.661114538839218, \"Test MAE: \", 34.04060460201007)\n",
      "(3, \"Train MAE: \", 32.1363386453074, \"Test MAE: \", 34.59095561385012)\n",
      "(4, \"Train MAE: \", 32.45203162035087, \"Test MAE: \", 34.884738258584576)\n",
      "(5, \"Train MAE: \", 33.16279586864789, \"Test MAE: \", 35.73930454094331)\n",
      "(6, \"Train MAE: \", 31.0284649941921, \"Test MAE: \", 33.269397891962065)\n",
      "(7, \"Train MAE: \", 30.51839467255816, \"Test MAE: \", 31.596391669832375)\n",
      "(8, \"Train MAE: \", 31.31327033484613, \"Test MAE: \", 33.6583217257427)\n",
      "(9, \"Train MAE: \", 31.030640652782424, \"Test MAE: \", 31.7152621742788)\n",
      "(10, \"Train MAE: \", 30.47125147899626, \"Test MAE: \", 30.8395278370906)\n",
      "(11, \"Train MAE: \", 30.46739342100721, \"Test MAE: \", 31.445936692316653)\n",
      "(12, \"Train MAE: \", 30.536060022422344, \"Test MAE: \", 31.32531242285673)\n",
      "(13, \"Train MAE: \", 30.234706213612263, \"Test MAE: \", 30.715517179692334)\n",
      "(14, \"Train MAE: \", 30.139356346654683, \"Test MAE: \", 30.060117113105573)\n",
      "(15, \"Train MAE: \", 30.04732317973228, \"Test MAE: \", 30.319428205605636)\n",
      "(16, \"Train MAE: \", 30.931037226970172, \"Test MAE: \", 32.22975283762126)\n",
      "(17, \"Train MAE: \", 30.787024389138693, \"Test MAE: \", 31.55780175404902)\n",
      "(18, \"Train MAE: \", 30.229797108131866, \"Test MAE: \", 30.250384890917)\n",
      "(19, \"Train MAE: \", 30.059168365899247, \"Test MAE: \", 30.032119370129436)\n",
      "(20, \"Train MAE: \", 30.259627578227484, \"Test MAE: \", 30.688684676409505)\n",
      "(21, \"Train MAE: \", 30.062636513400665, \"Test MAE: \", 30.195795410098214)\n",
      "(22, \"Train MAE: \", 30.4603016704195, \"Test MAE: \", 30.849878752688817)\n",
      "(23, \"Train MAE: \", 30.024784002532268, \"Test MAE: \", 30.159048074704465)\n",
      "(24, \"Train MAE: \", 29.992074161494173, \"Test MAE: \", 30.27611131269895)\n",
      "(25, \"Train MAE: \", 30.048216045620748, \"Test MAE: \", 30.03651455045435)\n",
      "(26, \"Train MAE: \", 29.938711420643056, \"Test MAE: \", 29.73624296213409)\n",
      "(27, \"Train MAE: \", 29.902061656736517, \"Test MAE: \", 29.473198564330993)\n",
      "(28, \"Train MAE: \", 29.936428356874067, \"Test MAE: \", 30.67598969429629)\n",
      "(29, \"Train MAE: \", 30.00905023825785, \"Test MAE: \", 30.622781949694307)\n",
      "(30, \"Train MAE: \", 29.906127881589228, \"Test MAE: \", 30.561715837586867)\n",
      "(31, \"Train MAE: \", 29.89395779345041, \"Test MAE: \", 30.606252424144564)\n",
      "(32, \"Train MAE: \", 31.987962195565544, \"Test MAE: \", 30.906525710287518)\n",
      "(33, \"Train MAE: \", 30.8874129251362, \"Test MAE: \", 30.628003349869978)\n",
      "(34, \"Train MAE: \", 29.905751891123636, \"Test MAE: \", 29.79011311700055)\n",
      "(35, \"Train MAE: \", 29.8875098813999, \"Test MAE: \", 29.918089175334234)\n",
      "(36, \"Train MAE: \", 29.854314680392047, \"Test MAE: \", 29.85020425517562)\n",
      "(37, \"Train MAE: \", 29.863516286871178, \"Test MAE: \", 29.867272130323236)\n",
      "(38, \"Train MAE: \", 29.80666362024769, \"Test MAE: \", 29.755788357106113)\n",
      "(39, \"Train MAE: \", 29.811543352563326, \"Test MAE: \", 29.83498623210848)\n",
      "(40, \"Train MAE: \", 29.801362828485995, \"Test MAE: \", 29.88019992740601)\n",
      "(41, \"Train MAE: \", 29.91644858627972, \"Test MAE: \", 30.003908448039336)\n",
      "(42, \"Train MAE: \", 29.845053715510367, \"Test MAE: \", 30.02649732104618)\n",
      "(43, \"Train MAE: \", 29.83925939643448, \"Test MAE: \", 30.00147714062961)\n",
      "(44, \"Train MAE: \", 29.846477518413725, \"Test MAE: \", 30.039802621933628)\n",
      "(45, \"Train MAE: \", 29.842100754297324, \"Test MAE: \", 30.083500223760996)\n",
      "(46, \"Train MAE: \", 29.868561769105757, \"Test MAE: \", 30.27648892387114)\n",
      "(47, \"Train MAE: \", 29.917981343854454, \"Test MAE: \", 29.916736345773682)\n",
      "(48, \"Train MAE: \", 29.840690940147898, \"Test MAE: \", 30.22315994613003)\n",
      "(49, \"Train MAE: \", 30.02045321587069, \"Test MAE: \", 30.583129378730714)\n",
      "(50, \"Train MAE: \", 29.930768291614314, \"Test MAE: \", 30.331414310934463)\n",
      "410.058175 seconds (698.96 M allocations: 33.308 GiB, 2.25% gc time)\n"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "function main()\n",
    "    hidden_layer_sizes = [10,10]\n",
    "    w =  rnnweights(10,10)\n",
    "    h = zeros(10)\n",
    "    dtrn = zip([x_train[:,i] for i in 1:size(x_train,2)],y_train)\n",
    "    dtst = zip([x_test[:,i] for i in 1:size(x_test,2)],y_test)\n",
    "    epochs = 50\n",
    "    learning_rate = 0.0001\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_loss = rnn_total_loss(w,dtrn)\n",
    "    test_loss = rnn_total_loss(w,dtst)\n",
    "    println((0,\"Train MAE: \",train_loss[1],\"Test MAE: \",test_loss[1]))\n",
    "    push!(train_losses,train_loss[1])\n",
    "    push!(test_losses,test_loss[1])\n",
    "    @time for epoch=1:epochs\n",
    "        train(w, dtrn; lr=learning_rate, epochs=1, grad=rnnlossgradient)\n",
    "        train_loss = rnn_total_loss(w,dtrn)\n",
    "        test_loss = rnn_total_loss(w,dtst)\n",
    "        println((epoch,\"Train MAE: \",train_loss[1],\"Test MAE: \",test_loss[1]))\n",
    "        push!(train_losses,train_loss[1])\n",
    "        push!(test_losses,test_loss[1])\n",
    "    end\n",
    "    return train_losses,test_losses\n",
    "end\n",
    "\n",
    "train_losses,test_losses = main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mPrecompiling module GR.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "plot(1:length(train_losses),[train_losses,test_losses],title=\"Training and Test loss for RNN\",label=[\"Train loss\" \"Test loss\"],lw=3)\n",
    "png(string(\"rnn_\",Dates.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
